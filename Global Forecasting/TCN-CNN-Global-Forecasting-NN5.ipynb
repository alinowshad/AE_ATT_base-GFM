{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c393d1b",
   "metadata": {
    "_cell_guid": "5c45f4f4-22c4-459a-afc6-200e310aeaec",
    "_uuid": "2142aeee-a89d-42c2-b9f7-22fc1c93c3d7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-04-02T10:07:06.445179Z",
     "iopub.status.busy": "2024-04-02T10:07:06.444656Z",
     "iopub.status.idle": "2024-04-02T10:07:23.776615Z",
     "shell.execute_reply": "2024-04-02T10:07:23.775055Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 17.345455,
     "end_time": "2024-04-02T10:07:23.780114",
     "exception": false,
     "start_time": "2024-04-02T10:07:06.434659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn-extra\r\n",
      "  Downloading scikit_learn_extra-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn-extra) (1.23.5)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn-extra) (1.11.2)\r\n",
      "Requirement already satisfied: scikit-learn>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn-extra) (1.2.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.1.0)\r\n",
      "Installing collected packages: scikit-learn-extra\r\n",
      "Successfully installed scikit-learn-extra-0.3.0\r\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26cad358",
   "metadata": {
    "_cell_guid": "c0b48c9e-a38d-4e61-9e68-974101442a02",
    "_uuid": "888f9285-12ff-4b27-85b2-ed07363c3cfb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-04-02T10:07:23.800543Z",
     "iopub.status.busy": "2024-04-02T10:07:23.799483Z",
     "iopub.status.idle": "2024-04-02T10:07:38.800598Z",
     "shell.execute_reply": "2024-04-02T10:07:38.798863Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 15.015075,
     "end_time": "2024-04-02T10:07:38.804180",
     "exception": false,
     "start_time": "2024-04-02T10:07:23.789105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rstl\r\n",
      "  Downloading rstl-0.1.3-py3-none-any.whl (5.2 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rstl) (1.23.5)\r\n",
      "Installing collected packages: rstl\r\n",
      "Successfully installed rstl-0.1.3\r\n"
     ]
    }
   ],
   "source": [
    "! pip install rstl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06c030b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T10:07:38.825868Z",
     "iopub.status.busy": "2024-04-02T10:07:38.825384Z",
     "iopub.status.idle": "2024-04-02T10:07:54.342210Z",
     "shell.execute_reply": "2024-04-02T10:07:54.340556Z"
    },
    "papermill": {
     "duration": 15.531715,
     "end_time": "2024-04-02T10:07:54.345817",
     "exception": false,
     "start_time": "2024-04-02T10:07:38.814102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tcn\r\n",
      "  Downloading keras_tcn-3.5.0-py3-none-any.whl (13 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras-tcn) (1.23.5)\r\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (from keras-tcn) (2.12.0)\r\n",
      "Requirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.10/site-packages (from keras-tcn) (0.21.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (23.5.26)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (0.4.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (0.2.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.51.3)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (3.9.0)\r\n",
      "Requirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (0.4.13)\r\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (2.12.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (16.0.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (21.3)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (3.20.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (68.0.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.16.0)\r\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (2.12.3)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (2.12.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (2.3.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (4.6.3)\r\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.14.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (0.32.0)\r\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons->keras-tcn) (2.13.3)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow->keras-tcn) (0.40.0)\r\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow->keras-tcn) (0.2.0)\r\n",
      "Requirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow->keras-tcn) (1.11.2)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2.20.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (1.0.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (3.4.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2.31.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (0.7.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2.3.7)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow->keras-tcn) (3.0.9)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (4.2.4)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (0.2.7)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (4.9)\r\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (1.26.15)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (1.3.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2023.7.22)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2.1.3)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (3.2.2)\r\n",
      "Installing collected packages: keras-tcn\r\n",
      "Successfully installed keras-tcn-3.5.0\r\n"
     ]
    }
   ],
   "source": [
    "! pip install keras-tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7e7a3c4",
   "metadata": {
    "_cell_guid": "cadda679-3007-4dcb-a795-df2e1a505d57",
    "_uuid": "60d97a7d-a1a8-437d-ad96-c1d853b99d54",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-04-02T10:07:54.371124Z",
     "iopub.status.busy": "2024-04-02T10:07:54.369826Z",
     "iopub.status.idle": "2024-04-02T10:08:09.433422Z",
     "shell.execute_reply": "2024-04-02T10:08:09.432014Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 15.080364,
     "end_time": "2024-04-02T10:08:09.436940",
     "exception": false,
     "start_time": "2024-04-02T10:07:54.356576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /opt/conda/lib/python3.10/site-packages (0.14.0)\r\n",
      "Requirement already satisfied: numpy>=1.18 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (1.23.5)\r\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.4 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (1.11.2)\r\n",
      "Requirement already satisfied: pandas>=1.0 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (2.0.3)\r\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (0.5.3)\r\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (21.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21.3->statsmodels) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0->statsmodels) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0->statsmodels) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0->statsmodels) (2023.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc1ea61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T10:08:09.461642Z",
     "iopub.status.busy": "2024-04-02T10:08:09.461138Z",
     "iopub.status.idle": "2024-04-02T10:08:24.360497Z",
     "shell.execute_reply": "2024-04-02T10:08:24.358787Z"
    },
    "papermill": {
     "duration": 14.915851,
     "end_time": "2024-04-02T10:08:24.363678",
     "exception": false,
     "start_time": "2024-04-02T10:08:09.447827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.23.5 in /opt/conda/lib/python3.10/site-packages (1.23.5)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy==1.23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "523efff7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T10:08:24.388010Z",
     "iopub.status.busy": "2024-04-02T10:08:24.387489Z",
     "iopub.status.idle": "2024-04-02T10:08:35.905395Z",
     "shell.execute_reply": "2024-04-02T10:08:35.903666Z"
    },
    "papermill": {
     "duration": 11.534335,
     "end_time": "2024-04-02T10:08:35.909030",
     "exception": false,
     "start_time": "2024-04-02T10:08:24.374695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tcn import TCN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b1e9e2",
   "metadata": {
    "_cell_guid": "3893e010-cd1e-4ec8-8d57-250fdb23c5b6",
    "_uuid": "a67cc23c-71b2-4d21-b9a0-b7e3c76cdde7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-04-02T10:08:35.934018Z",
     "iopub.status.busy": "2024-04-02T10:08:35.932975Z",
     "iopub.status.idle": "2024-04-02T10:08:38.546761Z",
     "shell.execute_reply": "2024-04-02T10:08:38.545344Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 2.629798,
     "end_time": "2024-04-02T10:08:38.549822",
     "exception": false,
     "start_time": "2024-04-02T10:08:35.920024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from tcn import TCN\n",
    "import csv\n",
    "import os\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import SpectralClustering\n",
    "# %tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import layers\n",
    "from keras.models import Sequential,Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import time\n",
    "print(tf.__version__)\n",
    "from keras.layers import MultiHeadAttention\n",
    "from keras.layers import Dense\n",
    "import gc\n",
    "from keras.layers import concatenate\n",
    "import csv\n",
    "import math\n",
    "import warnings\n",
    "import os\n",
    "# import xgboost as xgb\n",
    "warnings.filterwarnings('ignore')\n",
    "# import GPy, GPyOpt\n",
    "tfkl = tf.keras.layers\n",
    "tfk = tf.keras\n",
    "from rstl import STL\n",
    "from texttable import Texttable\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5118f602",
   "metadata": {
    "_cell_guid": "7486db45-fbd4-42ff-bc99-38fa57d29892",
    "_uuid": "b76fef40-4a45-4325-b068-c088747bb821",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-04-02T10:08:38.576193Z",
     "iopub.status.busy": "2024-04-02T10:08:38.575651Z",
     "iopub.status.idle": "2024-04-02T10:08:38.721085Z",
     "shell.execute_reply": "2024-04-02T10:08:38.719696Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.163271,
     "end_time": "2024-04-02T10:08:38.724137",
     "exception": false,
     "start_time": "2024-04-02T10:08:38.560866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_params(dataset_name = 'nn5'):\n",
    "    suilin_smape = False\n",
    "    dataset_path = \"nothing\"\n",
    "    if dataset_name == 'nn5':\n",
    "        raw_data = pd.read_csv('/kaggle/input/nn5-dataset/Fixed_nn5.csv',sep='delimeter',header=None)\n",
    "        features = pd.read_csv(\"/kaggle/input/hyndmankhsnn5features/fs_hyndman_Fixed_nn5 (1).csv\",sep=',', header=0)\n",
    "\n",
    "\n",
    "        lag = 150\n",
    "        look_forward = 56\n",
    "        batch_size = 1\n",
    "        epochs = 1\n",
    "        learning_rate = 0.0001\n",
    "        suilin_smape = False\n",
    "        frequency =7\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "    sample_overlap = look_forward - 1\n",
    "\n",
    "\n",
    "    raw_data = raw_data[0].str.split(',', expand=True)\n",
    "\n",
    "    raw_data = raw_data.to_numpy().astype('float64')\n",
    "    features = features.to_numpy().astype('float64')\n",
    "    dataset = []\n",
    "    for i in range(len(raw_data)):\n",
    "        dataset.append(raw_data[i][~np.isnan(raw_data[i])])\n",
    "\n",
    "\n",
    "    return dataset, features, lag, look_forward, sample_overlap, learning_rate, dataset_path, suilin_smape, frequency\n",
    "\n",
    "def normalize_dataset(dataset, look_forward ):\n",
    "    data_means = [];\n",
    "    for index in range(len(dataset)):\n",
    "    # Mean Noramlization\n",
    "        series_mean = np.mean(dataset[index][:len(dataset[index]) - look_forward]) # Train Mean: look_forward || Full Mean: Mean: look_forward = 0\n",
    "\n",
    "        if series_mean == 0:\n",
    "            series_mean = 0.001\n",
    "\n",
    "        data_means.append(series_mean)\n",
    "        dataset[index] = np.divide(dataset[index], series_mean)\n",
    "\n",
    "        # Log Transformation\n",
    "        dataset[index] = np.log(dataset[index] + 1)\n",
    "        #dataset[index] = np.log(dataset[index])\n",
    "\n",
    "\n",
    "    return dataset, np.array(data_means)\n",
    "\n",
    "def rescale_data_to_main_value(data, means, dataset_seasonal = []):\n",
    "\n",
    "    for index in range(len(data)):\n",
    "        # Revert Log Transformation\n",
    "        data[index] = np.e ** data[index]\n",
    "        data[index] = data[index] - 1\n",
    "\n",
    "        # Revert Mean Normalization\n",
    "        data[index] = means[index] * data[index]\n",
    "\n",
    "        if len(dataset_seasonal) != 0:\n",
    "            data[index] = data[index] + dataset_seasonal[index]\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "def normalize_feature_vectors(features):\n",
    "    #------------------- Z-score ----------------------#\n",
    "#     means = features.mean(0)\n",
    "#     stds = features.std(0)\n",
    "\n",
    "#     for i in range(len(features)):\n",
    "#         features[i] = (features[i] - means) / stds\n",
    "\n",
    "    #--------------------Min - Max---------------------#\n",
    "    minimum = features.min(0)\n",
    "    maximum = features.max(0)\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        features[i] = (features[i] - minimum) / (maximum - minimum)\n",
    "\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "#RMSE\n",
    "def root_mean_squared_error(actual, forecast, method = 'single_value'):\n",
    "    # Methods = single_value | per_series\n",
    "    if method == 'single_value':\n",
    "        #Flatten To One Vector\n",
    "        actual = actual.flatten()\n",
    "        forecast = forecast.flatten()\n",
    "\n",
    "        return np.sqrt(np.mean(np.square(actual - forecast)))\n",
    "    elif method == 'per_series':\n",
    "        rmses = []\n",
    "        for i in range(len(actual)):\n",
    "            rmses.append(np.sqrt(np.mean(np.square(actual[i] - forecast[i]))))\n",
    "\n",
    "        return rmses\n",
    "\n",
    "\"\"\"![YIy33.png](attachment:YIy33.png)\"\"\"\n",
    "\n",
    "#SMAPE\n",
    "def single_point_smape(actual, forecast, suilin_smape = False):\n",
    "    if suilin_smape == True:\n",
    "        epsilon = 0.1\n",
    "\n",
    "        return (np.sum(2 * np.abs(forecast - actual) / max((np.abs(actual) + np.abs(forecast))+ epsilon, 0.5 + epsilon)))\n",
    "    else:\n",
    "        return (np.sum(2 * np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast))))\n",
    "\n",
    "def smape(actual, forecast, method = 'single_value', suilin_smape = False):\n",
    "    # Methods = single_value | per_series\n",
    "    if method == 'single_value':\n",
    "        #Flatten To One Vector\n",
    "        actual = actual.flatten()\n",
    "        forecast = forecast.flatten()\n",
    "        sum_smape = 0\n",
    "        for i in range(len(actual)):\n",
    "            sum_smape += single_point_smape(actual[i], forecast[i], suilin_smape)\n",
    "        return 100 * sum_smape / len(actual)\n",
    "\n",
    "    elif method == 'per_series':\n",
    "        smapes = []\n",
    "        for i in range(len(actual)):\n",
    "            sum_smape = 0\n",
    "            for j in range(len(actual[i])):\n",
    "                sum_smape += single_point_smape(actual[i,j], forecast[i,j], suilin_smape)\n",
    "            smapes.append(100 * sum_smape / len(actual[i]))\n",
    "        return np.array(smapes)\n",
    "\n",
    "# Create Samples from DataSet\n",
    "def create_dataset(sample, look_back, look_forward, sample_overlap, dataset_seasonal):\n",
    "    if(sample_overlap >= look_forward or sample_overlap < 0): sample_overlap = look_forward - 1\n",
    "    if(look_forward == 1): sample_overlap = 0\n",
    "\n",
    "    dataX, dataY, dataY_seasonal = [], [], []\n",
    "    dataX_means, dataY_means = [], []\n",
    "    for i in range(0, len(sample) - look_back - look_forward+1, look_forward - sample_overlap):\n",
    "        dataX.append(sample[i:(i+look_back), 0])\n",
    "        dataY.append(sample[(i + look_back):(i + look_back + look_forward), 0])\n",
    "\n",
    "        dataY_seasonal.append(dataset_seasonal[(i + look_back):(i + look_back + look_forward)])\n",
    "\n",
    "\n",
    "    return np.array(dataX), np.array(dataY), np.array(dataY_seasonal)\n",
    "\n",
    "\n",
    "#RMSE\n",
    "def root_mean_squared_error(actual, forecast, method = 'single_value'):\n",
    "    # Methods = single_value | per_series\n",
    "    if method == 'single_value':\n",
    "        #Flatten To One Vector\n",
    "        actual = actual.flatten()\n",
    "        forecast = forecast.flatten()\n",
    "\n",
    "        return np.sqrt(np.mean(np.square(actual - forecast)))\n",
    "    elif method == 'per_series':\n",
    "        rmses = []\n",
    "        for i in range(len(actual)):\n",
    "            rmses.append(np.sqrt(np.mean(np.square(actual[i] - forecast[i]))))\n",
    "\n",
    "        return rmses\n",
    "\n",
    "\"\"\"![YIy33.png](attachment:YIy33.png)\"\"\"\n",
    "def single_point_mase(actual, forecast, insample, frequency) :\n",
    "    # print(\"HHHHHHHHHHHHHHHHHHHHHHHHHHHH\")\n",
    "    return np.mean(np.abs(forecast - actual)) / np.mean(np.abs(insample[:-frequency] - insample[frequency:]))\n",
    "\n",
    "\n",
    "def mase(actual, forecast, insample,frequency):\n",
    "    print(\"shapes\",actual.shape,forecast.shape,insample.shape)\n",
    "    # print(insample)\n",
    "    MASE = []\n",
    "    for i in range(len(actual)):\n",
    "        sum_MASE= 0\n",
    "        for j in range(len(actual[i])):\n",
    "            sum_MASE += single_point_mase(actual[i, j], forecast[i, j],insample[i][:-len(actual[i])], frequency)\n",
    "        MASE.append(sum_MASE / len(actual[i]))\n",
    "    return np.array(MASE)\n",
    "\n",
    "\n",
    "def mase_val(actual, forecast, insample,frequency):\n",
    "    print(\"shapes\",actual.shape,forecast.shape,insample.shape)\n",
    "    # print(insample)\n",
    "    MASE = []\n",
    "    for i in range(len(actual)):\n",
    "        sum_MASE= 0\n",
    "        for j in range(len(actual[i])):\n",
    "            sum_MASE += single_point_mase(actual[i, j], forecast[i, j],insample[i][:-2*len(actual[i])], frequency)\n",
    "        MASE.append(sum_MASE / len(actual[i]))\n",
    "    return np.array(MASE)\n",
    "\n",
    "\n",
    "#SMAPE\n",
    "def single_point_smape(actual, forecast, suilin_smape = False):\n",
    "    if suilin_smape == True:\n",
    "        epsilon = 0.1\n",
    "\n",
    "        return (np.sum(2 * np.abs(forecast - actual) / max((np.abs(actual) + np.abs(forecast))+ epsilon, 0.5 + epsilon)))\n",
    "    else:\n",
    "        return (np.sum(2 * np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast))))\n",
    "\n",
    "def smape(actual, forecast, method = 'single_value', suilin_smape = False):\n",
    "    # Methods = single_value | per_series\n",
    "    if method == 'single_value':\n",
    "        #Flatten To One Vector\n",
    "        actual = actual.flatten()\n",
    "        forecast = forecast.flatten()\n",
    "        sum_smape = 0\n",
    "        for i in range(len(actual)):\n",
    "            sum_smape += single_point_smape(actual[i], forecast[i], suilin_smape)\n",
    "        return 100 * sum_smape / len(actual)\n",
    "\n",
    "    elif method == 'per_series':\n",
    "        smapes = []\n",
    "        for i in range(len(actual)):\n",
    "            sum_smape = 0\n",
    "            for j in range(len(actual[i])):\n",
    "                sum_smape += single_point_smape(actual[i,j], forecast[i,j], suilin_smape)\n",
    "            smapes.append(100 * sum_smape / len(actual[i]))\n",
    "        return np.array(smapes)\n",
    "\n",
    "# Create Samples from DataSet\n",
    "def create_dataset(sample, look_back, look_forward, sample_overlap, dataset_seasonal):\n",
    "    if(sample_overlap >= look_forward or sample_overlap < 0): sample_overlap = look_forward - 1\n",
    "    if(look_forward == 1): sample_overlap = 0\n",
    "\n",
    "    dataX, dataY, dataY_seasonal = [], [], []\n",
    "    dataX_means, dataY_means = [], []\n",
    "    for i in range(0, len(sample) - look_back - look_forward+1, look_forward - sample_overlap):\n",
    "        dataX.append(sample[i:(i+look_back), 0])\n",
    "        dataY.append(sample[(i + look_back):(i + look_back + look_forward), 0])\n",
    "\n",
    "        dataY_seasonal.append(dataset_seasonal[(i + look_back):(i + look_back + look_forward)])\n",
    "\n",
    "\n",
    "    return np.array(dataX), np.array(dataY), np.array(dataY_seasonal)\n",
    "\n",
    "def create_sample(look_forward,sample_seasonal,dataX, dataY, data_mean, dataY_seasonal,frequency):\n",
    "    test_size=1\n",
    "    val_size=1\n",
    "    # print(frequency)\n",
    "    # print(sample_seasonal)\n",
    "    # test_size=int(len(dataX) * testsize)\n",
    "    # val_size=int((len(dataX) - test_size) * valsize)\n",
    "\n",
    "    train_size=(len(dataX)-test_size)\n",
    "\n",
    "    trainX, testX = dataX[0:train_size,:], dataX[train_size:,:]\n",
    "    trainY, testY = dataY[0:train_size,:], dataY[train_size:,:]\n",
    "\n",
    "    valX, valY = trainX[train_size-val_size:train_size,:],trainY[train_size-val_size:train_size, :]\n",
    "\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0],1, trainX.shape[1]))\n",
    "    valX = np.reshape(valX, (valX.shape[0],1, valX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0],1, testX.shape[1]))\n",
    "\n",
    "    val_means = np.full(len(valY), data_mean)\n",
    "    test_means = np.full(len(testY), data_mean)\n",
    "\n",
    "    val_seasonal = dataY_seasonal[train_size-val_size:train_size, :]\n",
    "    # print(\"len val\",len(val_seasonal))\n",
    "    # print(\"val\",val_seasonal)\n",
    "\n",
    "    train=dataY_seasonal[:train_size, :]\n",
    "    train=train.reshape(-1,1)\n",
    "    train2=train[:len(train):len(valY[0])]\n",
    "    # print(train2.reshape(1,-1))\n",
    "    # print(len(train2))\n",
    "\n",
    "    test2 = []\n",
    "\n",
    "\n",
    "    # modl = pm.auto_arima(train2, trace=False,seasonal=True, stepwise=False, information_criterion='aic',)\n",
    "    # preds = modl.predict(n_periods=len(valY[0]))\n",
    "    # print(\"xx\",train2.shape)\n",
    "    sample_size = len(sample_seasonal.flatten()) - look_forward\n",
    "\n",
    "    train3=sample_seasonal[:sample_size].flatten()\n",
    "\n",
    "    if frequency!=None:\n",
    "        if len(train3.flatten()) > frequency*2:\n",
    "            sp = frequency\n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten()), seasonal_periods=sp,trend='add', seasonal='add').fit()\n",
    "\n",
    "        elif len(train3.flatten())<frequency*2 and len(train3.flatten())>frequency :\n",
    "            sp = int(frequency/2)\n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten()), seasonal_periods=sp, trend='add',\n",
    "                                        seasonal='add').fit()\n",
    "        else:\n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten())).fit()\n",
    "\n",
    "        preds2 = fit1.forecast(steps=len(valY[0])).values.reshape(1,-1)\n",
    "\n",
    "        # print(\"preds\",preds2,type(preds2))\n",
    "    else:\n",
    "        preds2=np.zeros(look_forward)\n",
    "    for i in range(0,len(val_seasonal[0])):\n",
    "        test2.append(val_seasonal[0][len(val_seasonal[0])-1])\n",
    "\n",
    "    # print(\"datay_seasonal\",dataY_seasonal)\n",
    "\n",
    "    test_seasonal_y = dataY_seasonal[train_size:,:]\n",
    "    # print(\"test\",test_seasonal_y)\n",
    "    # print(\"train2\",train2.flatten())\n",
    "    # np.savetxt('train.csv', train2.flatten(), delimiter=', ')\n",
    "\n",
    "    return np.array(trainX),np.array(valX),np.array(testX),np.array(trainY),np.array(valY),np.array(testY), test_means, val_means, val_seasonal,test_seasonal_y, preds2\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess Data For Sampling\n",
    "def all_pre_process(all_dataset, lag, look_forward, sample_overlap, data_means, dataset_seasonal,frequency):\n",
    "    look_back = lag\n",
    "\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "\n",
    "    valX = []\n",
    "    valY = []\n",
    "\n",
    "    testX = []\n",
    "    testY = []\n",
    "\n",
    "    all_test_means = []\n",
    "    all_val_means = []\n",
    "\n",
    "    all_test_seasonals = []\n",
    "    all_test_seasonals2 = []\n",
    "    all_val_seasonals = []\n",
    "\n",
    "    for index in range(len(all_dataset)):\n",
    "        sample = np.array(all_dataset[index])\n",
    "        sample = sample.reshape(sample.shape[0], 1)\n",
    "        # sample_seasonal=np.array(dataset_seasonal[index])\n",
    "\n",
    "        dataX_s, dataY_s, dataY_seasonal = create_dataset(sample, look_back, look_forward, sample_overlap, dataset_seasonal[index])\n",
    "        temp_trainX, temp_valX, temp_testX, temp_trainY, temp_valY, temp_testY, test_means, val_means, val_seasonal, test_seasonal,test2 = create_sample(look_forward,dataset_seasonal[index],dataX_s,dataY_s,data_means[index], dataY_seasonal,frequency)\n",
    "\n",
    "        trainX = trainX + temp_trainX.tolist()\n",
    "        trainY = trainY + temp_trainY.tolist()\n",
    "\n",
    "        valX = valX + temp_valX.tolist()\n",
    "        valY = valY + temp_valY.tolist()\n",
    "\n",
    "        testX = testX + temp_testX.tolist()\n",
    "        testY = testY + temp_testY.tolist()\n",
    "\n",
    "        all_test_means = all_test_means + test_means.tolist()\n",
    "        all_val_means = all_val_means + val_means.tolist()\n",
    "        all_test_seasonals = all_test_seasonals +  test_seasonal.tolist()\n",
    "        all_test_seasonals2 = all_test_seasonals2 +test2.tolist()  # test_seasonal.tolist() #\"NOTE\"\n",
    "        all_val_seasonals = all_val_seasonals + val_seasonal.tolist()\n",
    "\n",
    "\n",
    "    return np.array(trainX), np.array(valX), np.array(testX), np.array(trainY), np.array(valY), np.array(testY), np.array(all_test_means), np.array(all_val_means), np.array(all_val_seasonals), np.array(all_test_seasonals),np.array(all_test_seasonals2)\n",
    "\n",
    "def save_prediction_result(data, dataset_name = 'cif-6', dataset_path = ''):\n",
    "    if dataset_name == '':\n",
    "        filename = dataset_name + '-results.csv'\n",
    "    else:\n",
    "        filename = dataset_path + '/' + dataset_name + '-results.csv'\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, sep=',',index=False,header=False)\n",
    "\n",
    "\n",
    "# Preprocess Data For Sampling\n",
    "def all_pre_process(all_dataset, lag, look_forward, sample_overlap, data_means, dataset_seasonal,frequency):\n",
    "    look_back = lag\n",
    "\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "\n",
    "    valX = []\n",
    "    valY = []\n",
    "\n",
    "    testX = []\n",
    "    testY = []\n",
    "\n",
    "    all_test_means = []\n",
    "    all_val_means = []\n",
    "\n",
    "    all_test_seasonals = []\n",
    "    all_test_seasonals2 = []\n",
    "    all_val_seasonals = []\n",
    "\n",
    "    for index in range(len(all_dataset)):\n",
    "        sample = np.array(all_dataset[index])\n",
    "        sample = sample.reshape(sample.shape[0], 1)\n",
    "        # sample_seasonal=np.array(dataset_seasonal[index])\n",
    "\n",
    "        dataX_s, dataY_s, dataY_seasonal = create_dataset(sample, look_back, look_forward, sample_overlap, dataset_seasonal[index])\n",
    "        temp_trainX, temp_valX, temp_testX, temp_trainY, temp_valY, temp_testY, test_means, val_means, val_seasonal, test_seasonal,test2 = create_sample(look_forward,dataset_seasonal[index],dataX_s,dataY_s,data_means[index], dataY_seasonal,frequency)\n",
    "\n",
    "        trainX = trainX + temp_trainX.tolist()\n",
    "        trainY = trainY + temp_trainY.tolist()\n",
    "\n",
    "        valX = valX + temp_valX.tolist()\n",
    "        valY = valY + temp_valY.tolist()\n",
    "\n",
    "        testX = testX + temp_testX.tolist()\n",
    "        testY = testY + temp_testY.tolist()\n",
    "\n",
    "        all_test_means = all_test_means + test_means.tolist()\n",
    "        all_val_means = all_val_means + val_means.tolist()\n",
    "        all_test_seasonals = all_test_seasonals +  test_seasonal.tolist()\n",
    "        all_test_seasonals2 = all_test_seasonals2 +test2.tolist()  # test_seasonal.tolist() #\"NOTE\"\n",
    "        all_val_seasonals = all_val_seasonals + val_seasonal.tolist()\n",
    "\n",
    "\n",
    "    return np.array(trainX), np.array(valX), np.array(testX), np.array(trainY), np.array(valY), np.array(testY), np.array(all_test_means), np.array(all_val_means), np.array(all_val_seasonals), np.array(all_test_seasonals),np.array(all_test_seasonals2)\n",
    "\n",
    "\n",
    "def save_prediction_result(data, dataset_name = 'cif-6', dataset_path = ''):\n",
    "    if dataset_name == '':\n",
    "        filename = dataset_name + '-results.csv'\n",
    "    else:\n",
    "        filename = dataset_path + '/' + dataset_name + '-results.csv'\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, sep=',',index=False,header=False)\n",
    "\n",
    "\"\"\"#@main\"\"\"\n",
    "\n",
    "# Main Work & Functionality\n",
    "def run_model_test(dataset, data_means, dataset_seasonal, dataset_name, cluster_lable, lag, look_forward, sample_overlap, batch_size, epochs, learning_rate, suilin_smape, dataset_path,frequency, use_saved_model = False, save_trained_model = False):\n",
    "    print(\"len dataset\",len(dataset))\n",
    "    # Initialize Look Forward & Back\n",
    "    look_back=lag\n",
    "    calculations_method = 'per_series' # single_value | per_series\n",
    "\n",
    "    trainX, valX, testX, trainY, valY, testY, test_means, val_means, val_seasonal, test_seasonal,test_seasonal2 = all_pre_process(dataset, lag, look_forward, sample_overlap, data_means, dataset_seasonal,frequency)\n",
    "\n",
    "\n",
    "\n",
    "    # Get Model From Local Saved File\n",
    "    if use_saved_model == True:\n",
    "        if os.path.exists(dataset_path + '/' + dataset_name + '-model-cluster-' + str(cluster_lable)) == True:\n",
    "            model = keras.models.load_model(dataset_path + '/' + dataset_name  + '-model-cluster-' + str(cluster_lable))\n",
    "\n",
    "            val_prediction_results = model.predict([valX],batch_size=16, verbose=0)\n",
    "\n",
    "            val_RMSE = root_mean_squared_error(valY, val_prediction_results, calculations_method)\n",
    "            val_SMAPE = smape(valY, val_prediction_results, calculations_method, suilin_smape)\n",
    "\n",
    "            ######################################################\n",
    "            test_prediction_results = model.predict([testX],batch_size=16, verbose=0)\n",
    "\n",
    "            test_RMSE = root_mean_squared_error(testY, test_prediction_results, calculations_method)\n",
    "            test_SMAPE = smape(testY, test_prediction_results, calculations_method, suilin_smape)\n",
    "\n",
    "        else:\n",
    "            use_saved_model = False\n",
    "            save_trained_model = True\n",
    "\n",
    "    # Train Model From Scratch\n",
    "    if use_saved_model == False:\n",
    "\n",
    "        dense_neuron = 100\n",
    "        denselayer_activation = 'linear' #None\n",
    "        output_activation = 'linear' #'linear'\n",
    "\n",
    "        print(\"---------------------------------------------------------------------\")\n",
    "        print(\"lag\", lag)\n",
    "        print(\"look_forward\", look_forward)\n",
    "        print(\"sample overlap\", sample_overlap)\n",
    "        print(\"trainshape\", trainX.shape)\n",
    "        print(\"valshape\", valX.shape)\n",
    "        print(\"testshape\", testX.shape)\n",
    "        print(learning_rate, dense_neuron, denselayer_activation, output_activation)\n",
    "\n",
    "\n",
    "\n",
    "        validation_loss=[]\n",
    "        test_loss=[]\n",
    "        iter = 1\n",
    "        for j in range(iter):\n",
    "            # #---------------------------------------Input Layer------------------------------------------#\n",
    "            input_layer = layers.Input(shape = (1, lag,), name = \"Input-Layer\")\n",
    "\n",
    "\n",
    "\n",
    "            multi_head_attention_layer = TCN(return_sequences=True,dilations=[1, 2, 4, 8])(input_layer)\n",
    "            conv = keras.layers.Conv1D(64,\n",
    "                              strides=2,\n",
    "                              kernel_size=4,\n",
    "                              activation=None,\n",
    "                              padding=\"same\",)(multi_head_attention_layer)#multi_head_attention_layer\n",
    "            conv2 = keras.layers.Conv1D(16,\n",
    "                              strides=2,\n",
    "                              kernel_size=4,\n",
    "                              activation=None,\n",
    "                              padding=\"same\",)(conv)\n",
    "            flatten_layer2=keras.layers.Flatten(name=\"Flatten-Layer2\")(conv2)\n",
    "\n",
    "            dense_layer1 = Dense(\n",
    "                dense_neuron,\n",
    "                activation = denselayer_activation,\n",
    "                name = \"Fully-Connected-Layer\")(flatten_layer2)\n",
    "\n",
    "            dense_layer2 = Dense(\n",
    "                look_forward,\n",
    "                activation = None,\n",
    "                name = \"Output-Layer\")(dense_layer1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Create Model\n",
    "            model = Model(inputs = [input_layer], outputs = dense_layer2)\n",
    "\n",
    "            # Optimizer\n",
    "            opt=tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "            # Compile Model\n",
    "            model.compile(loss=\"mse\",optimizer=opt,metrics=[\"mse\"])\n",
    "\n",
    "            for k in range(epochs): #epochs\n",
    "                history = model.fit([trainX], trainY, validation_data=([valX, valY]),\n",
    "                    verbose = 0,\n",
    "                    batch_size = batch_size,\n",
    "            ).history\n",
    "\n",
    "                val_prediction_results = model.predict([valX],batch_size=16, verbose=0)\n",
    "\n",
    "                val_RMSE = root_mean_squared_error(valY, val_prediction_results, calculations_method)\n",
    "                val_SMAPE = smape(valY, val_prediction_results, calculations_method, suilin_smape)\n",
    "\n",
    "                ######################################################\n",
    "                test_prediction_results = model.predict([testX],batch_size=16, verbose=0)\n",
    "\n",
    "                test_RMSE = root_mean_squared_error(testY, test_prediction_results, calculations_method)\n",
    "                test_SMAPE = smape(testY, test_prediction_results, calculations_method, suilin_smape)\n",
    "\n",
    "                validation_loss.append(np.mean(val_RMSE))\n",
    "                test_loss.append(np.mean(test_RMSE))\n",
    "\n",
    "            K.clear_session()\n",
    "            gc.collect()\n",
    "\n",
    "            # Save model to a file if wanted\n",
    "            if save_trained_model == True:\n",
    "                model.save(dataset_path + '/' + dataset_name + '-model-cluster-' + str(cluster_lable))\n",
    "\n",
    "            model=None\n",
    "            del history\n",
    "\n",
    "    rescaled_valY = rescale_data_to_main_value(valY, val_means, val_seasonal)\n",
    "    rescaled_trainY = rescale_data_to_main_value(dataset, data_means, dataset_seasonal)\n",
    "    rescaled_val_prediction_results = rescale_data_to_main_value(val_prediction_results, val_means,val_seasonal) #####\n",
    "    val_mase = mase_val(rescaled_valY,rescaled_val_prediction_results,rescaled_trainY,frequency)\n",
    "    val_SMAPE = smape(rescaled_valY, rescaled_val_prediction_results, calculations_method, suilin_smape)\n",
    "    val_RMSE = root_mean_squared_error(rescaled_valY, rescaled_val_prediction_results, calculations_method)\n",
    "\n",
    "\n",
    "    rescaled_testY = rescale_data_to_main_value(testY, test_means, test_seasonal)\n",
    "    rescaled_test_prediction_results = rescale_data_to_main_value(test_prediction_results, test_means,test_seasonal2 ) ###,test_seasonal\n",
    "    test_mase = mase(rescaled_testY,rescaled_test_prediction_results,rescaled_trainY,frequency)\n",
    "    test_SMAPE = smape(rescaled_testY, rescaled_test_prediction_results, calculations_method)\n",
    "    test_RMSE = root_mean_squared_error(rescaled_testY, rescaled_test_prediction_results, calculations_method)\n",
    "\n",
    "    results = {\n",
    "            'val_SMAPE': val_SMAPE,\n",
    "            'val_RMSE': val_RMSE,\n",
    "            'val_MASE': val_mase,\n",
    "            'test_SMAPE': test_SMAPE,\n",
    "            'test_RMSE': test_RMSE,\n",
    "            'test_MASE': test_mase\n",
    "        }\n",
    "    #############################################\n",
    "\n",
    "    return results\n",
    "\n",
    "def cluster_series(features, number_of_clusters=2):\n",
    "    clustered = SpectralClustering(n_clusters=number_of_clusters,assign_labels = 'discretize',\n",
    "                                   random_state = 0).fit(features)\n",
    "    print(\"Spectral clustering\")\n",
    "    print('silhouette_score -------->', silhouette_score(features, clustered.labels_))\n",
    "    return clustered.labels_\n",
    "\n",
    "def stl_decomposition(dataset, frequency,look):\n",
    "    seasonal = []\n",
    "    trend = []\n",
    "    for index in range(len(dataset)):\n",
    "        if frequency != None:\n",
    "            stl = STL(dataset[index][:len(dataset[index]) - look], frequency, \"periodic\")\n",
    "            stl_tot = STL(dataset[index], frequency, \"periodic\")\n",
    "\n",
    "            seasonal.append(np.concatenate([stl.seasonal,stl_tot.seasonal[-look:]]))\n",
    "            trend.append(np.concatenate([stl.trend,stl_tot.trend[-look:]]))\n",
    "            # deseason_part=dataset[index][:len(dataset[index]) - look] - stl.seasonal\n",
    "            # d=np.concatenate([deseason_part,dataset[index][-look:]])\n",
    "            dataset[index]=dataset[index]-np.concatenate([stl.seasonal,stl_tot.seasonal[-look:]])\n",
    "        else:\n",
    "            seasonal.append(np.zeros((dataset[index].shape)))\n",
    "            trend.append(np.zeros((dataset[index].shape)))\n",
    "\n",
    "    return dataset, np.array(seasonal), np.array(trend)\n",
    "\n",
    "def run_local_models(dataset_name, number_of_clusters=2, AEName='LSTM', Dim=8, epochs = 20, batch =20, use_saved_model = False, save_trained_model = False, run=1):\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print('dataset: ', dataset_name)\n",
    "    batch_size = batch\n",
    "    epochs = epochs\n",
    "    # Prepare & Read Data\n",
    "    dataset, features, lag, look_forward, sample_overlap, learning_rate, dataset_path, suilin_smape, frequency = get_dataset_params(dataset_name)\n",
    "\n",
    "    # Normalize Data\n",
    "    dataset, data_means = normalize_dataset(dataset, look_forward)\n",
    "\n",
    "    dataset, seasonal, trend = stl_decomposition(dataset, frequency, look_forward)\n",
    "\n",
    "    # Normalize Features\n",
    "    features = normalize_feature_vectors(features)\n",
    "\n",
    "    # Cluster Series Based On Feature Vectors (Feature Based Clustering)\n",
    "    if number_of_clusters == 1:\n",
    "        clusters = np.zeros(len(features))\n",
    "    else:\n",
    "        clusters = cluster_series(features, number_of_clusters)\n",
    "\n",
    "    dataset = np.array(dataset)\n",
    "\n",
    "    results = {\n",
    "        'val_SMAPE': np.array([]),\n",
    "        'val_RMSE': np.array([]),\n",
    "        'val_MASE': np.array([]),\n",
    "        'test_SMAPE': np.array([]),\n",
    "        'test_RMSE': np.array([]),\n",
    "        'test_MASE': np.array([])\n",
    "    }\n",
    "\n",
    "    # Loop Trough Clusters\n",
    "    for cluster_lable in range(number_of_clusters):\n",
    "        idx = [x for x in range(len(clusters)) if clusters[x] == cluster_lable]\n",
    "        cluster_dataset = np.array(dataset)[idx]\n",
    "        cluster_dataset_means = data_means[idx]\n",
    "        cluster_dataset_seasonal = seasonal[idx]\n",
    "\n",
    "        result = run_model_test(cluster_dataset, cluster_dataset_means, cluster_dataset_seasonal, dataset_name, cluster_lable, lag, look_forward, sample_overlap, batch_size, epochs, learning_rate, suilin_smape, dataset_path, frequency, use_saved_model, save_trained_model)\n",
    "\n",
    "        results = {\n",
    "            'val_SMAPE': np.concatenate((results['val_SMAPE'], result['val_SMAPE'])),\n",
    "            'val_RMSE': np.concatenate((results['val_RMSE'], result['val_RMSE'])),\n",
    "            'val_MASE': np.concatenate((results['val_MASE'], result['val_MASE'])),\n",
    "            'test_SMAPE': np.concatenate((results['test_SMAPE'], result['test_SMAPE'])),\n",
    "            'test_RMSE': np.concatenate((results['test_RMSE'], result['test_RMSE'])),\n",
    "            'test_MASE': np.concatenate((results['test_MASE'], result['test_MASE']))\n",
    "        }\n",
    "    \n",
    "    filename_val_smape = \"val_SMAPE_\"+str(run)+\"_\"+dataset_name+\"_\"+str(batch_size)+\"_\"+str(epochs)+\".npy\"\n",
    "    filename_val_rmse = \"val_RMSE_\"+str(run)+\"_\"+dataset_name+\"_\"+str(batch_size)+\"_\"+str(epochs)+\".npy\"\n",
    "    filename_val_mase = \"val_MASE_\"+str(run)+\"_\"+dataset_name+\"_\"+str(batch_size)+\"_\"+str(epochs)+\".npy\"\n",
    "    filename_test_smape = \"test_SMAPE_\"+str(run)+\"_\"+dataset_name+\"_\"+str(batch_size)+\"_\"+str(epochs)+\".npy\"\n",
    "    filename_test_rmse = \"test_RMSE_\"+str(run)+\"_\"+dataset_name+\"_\"+str(batch_size)+\"_\"+str(epochs)+\".npy\"\n",
    "    filename_test_mase = \"test_MASE_\"+str(run)+\"_\"+dataset_name+\"_\"+str(batch_size)+\"_\"+str(epochs)+\".npy\"\n",
    "\n",
    "    # Save the NumPy array to the file\n",
    "    np.save(filename_val_smape, results['val_SMAPE'])\n",
    "    np.save(filename_val_rmse, results['val_RMSE'])\n",
    "    np.save(filename_val_mase, results['val_MASE'])\n",
    "    np.save(filename_test_smape, results['test_SMAPE'])\n",
    "    np.save(filename_test_rmse, results['test_RMSE'])\n",
    "    np.save(filename_test_mase, results['test_MASE'])\n",
    "    t = Texttable()\n",
    "    print('\\n\\n#------------------------------------Scaled------------------------------------#')\n",
    "    t.add_rows([\n",
    "        ['Index', 'Mean sMAPE', 'Median sMAPE', 'Mean RMSE', 'Median RMSE', 'Mean MASE'],\n",
    "        ['Validate', np.mean(results['val_SMAPE']), np.median(results['val_SMAPE']), np.mean(results['val_RMSE']), np.median(results['val_RMSE']), np.mean(results['val_MASE'])],\n",
    "        ['Test', np.mean(results['test_SMAPE']), np.median(results['test_SMAPE']), np.mean(results['test_RMSE']), np.median(results['test_RMSE']), np.mean(results['test_MASE'])]\n",
    "    ])\n",
    "    \n",
    "    print(t.draw())\n",
    "    t = Texttable()\n",
    "    t.add_rows([\n",
    "        ['dataset Name', 'Run', 'N.Epochs', 'Batch Size', 'Auto Encoder', 'Latent Dimension'],\n",
    "        [dataset_name, run, epochs,batch_size, AEName, Dim]\n",
    "    ])\n",
    "    print(t.draw())\n",
    "    initial_data = [\n",
    "    ['dataset Name', 'Run', 'N.Epochs', 'Batch Size', 'Auto Encoder', 'Latent Dimension', 'Index', 'Mean sMAPE', 'Median sMAPE', 'Mean RMSE', 'Median RMSE', 'Mean MASE'],\n",
    "    [dataset_name, run, epochs, batch_size, AEName, Dim, 'Validate', np.mean(results['val_SMAPE']), np.median(results['val_SMAPE']), np.mean(results['val_RMSE']), np.median(results['val_RMSE']), np.mean(results['val_MASE'])],\n",
    "    [dataset_name, run, epochs, batch_size, AEName, Dim, 'Test', np.mean(results['test_SMAPE']), np.median(results['test_SMAPE']), np.mean(results['test_RMSE']), np.median(results['test_RMSE']), np.mean(results['test_MASE'])]\n",
    "    ]# Write the header and initial data to the CSV file\n",
    "\n",
    "    return results, initial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10422b0e",
   "metadata": {
    "_cell_guid": "e40c1f39-d0c9-4d37-96d3-ea0a8ddce4be",
    "_uuid": "6a170f26-ea7d-4a83-a126-cb72080dd225",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-04-02T10:08:38.748913Z",
     "iopub.status.busy": "2024-04-02T10:08:38.748371Z",
     "iopub.status.idle": "2024-04-02T10:08:38.758000Z",
     "shell.execute_reply": "2024-04-02T10:08:38.756737Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025504,
     "end_time": "2024-04-02T10:08:38.760827",
     "exception": false,
     "start_time": "2024-04-02T10:08:38.735323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nn5(AEName=\"Single\", Dim=0, run=1):\n",
    "    \n",
    "    ds_names = [AEName, [50], [5, 10, 15], Dim]\n",
    "    file_mode = 'w'\n",
    "    csv_file = \"NN5_results_\"+AEName+\"_\"+str(Dim)+\".csv\"\n",
    "    if os.path.exists(csv_file):\n",
    "        file_mode = 'a'\n",
    "    else:\n",
    "        file_mode = 'w'\n",
    "\n",
    "    with open(csv_file, mode=file_mode, newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for epoch in ds_names[1]:\n",
    "            for batch in ds_names[2]:\n",
    "    \n",
    "                hospital_results, initial_data = run_local_models(dataset_name = 'nn5', number_of_clusters = 1, AEName=ds_names[0], Dim=ds_names[3], epochs = epoch, batch = batch, use_saved_model = False, save_trained_model = False, run=run)\n",
    "                writer.writerows(initial_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "577b64f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T10:08:38.785609Z",
     "iopub.status.busy": "2024-04-02T10:08:38.785072Z",
     "iopub.status.idle": "2024-04-02T19:57:03.031405Z",
     "shell.execute_reply": "2024-04-02T19:57:03.030028Z"
    },
    "papermill": {
     "duration": 35304.262334,
     "end_time": "2024-04-02T19:57:03.034499",
     "exception": false,
     "start_time": "2024-04-02T10:08:38.772165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "dataset:  nn5\n",
      "len dataset 111\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (64935, 1, 150)\n",
      "valshape (111, 1, 150)\n",
      "testshape (111, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE | Mean MASE |\n",
      "+==========+============+==============+===========+=============+===========+\n",
      "| Validate | 18.720     | 18.275       | 4.516     | 4.234       | 0.780     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "| Test     | 19.683     | 18.892       | 4.729     | 4.506       | 0.832     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "| dataset Name | Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+==============+=====+==========+============+==============+==================+\n",
      "| nn5          | 0   | 50       | 5          | Single       | 0                |\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "len dataset 111\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (64935, 1, 150)\n",
      "valshape (111, 1, 150)\n",
      "testshape (111, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE | Mean MASE |\n",
      "+==========+============+==============+===========+=============+===========+\n",
      "| Validate | 19.220     | 18.807       | 4.589     | 4.359       | 0.801     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "| Test     | 19.555     | 19.001       | 4.684     | 4.386       | 0.826     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "| dataset Name | Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+==============+=====+==========+============+==============+==================+\n",
      "| nn5          | 0   | 50       | 10         | Single       | 0                |\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "len dataset 111\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (64935, 1, 150)\n",
      "valshape (111, 1, 150)\n",
      "testshape (111, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE | Mean MASE |\n",
      "+==========+============+==============+===========+=============+===========+\n",
      "| Validate | 18.715     | 17.962       | 4.468     | 4.224       | 0.777     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "| Test     | 20.237     | 19.620       | 4.810     | 4.440       | 0.855     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "| dataset Name | Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+==============+=====+==========+============+==============+==================+\n",
      "| nn5          | 0   | 50       | 15         | Single       | 0                |\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "**************************************************\n",
      "dataset:  nn5\n",
      "len dataset 111\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (64935, 1, 150)\n",
      "valshape (111, 1, 150)\n",
      "testshape (111, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE | Mean MASE |\n",
      "+==========+============+==============+===========+=============+===========+\n",
      "| Validate | 18.470     | 17.675       | 4.445     | 4.250       | 0.765     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "| Test     | 19.748     | 19.197       | 4.738     | 4.502       | 0.833     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "| dataset Name | Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+==============+=====+==========+============+==============+==================+\n",
      "| nn5          | 1   | 50       | 5          | Single       | 0                |\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "len dataset 111\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (64935, 1, 150)\n",
      "valshape (111, 1, 150)\n",
      "testshape (111, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE | Mean MASE |\n",
      "+==========+============+==============+===========+=============+===========+\n",
      "| Validate | 18.947     | 18.413       | 4.497     | 4.320       | 0.788     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "| Test     | 19.264     | 18.536       | 4.619     | 4.334       | 0.812     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "| dataset Name | Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+==============+=====+==========+============+==============+==================+\n",
      "| nn5          | 1   | 50       | 10         | Single       | 0                |\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "len dataset 111\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (64935, 1, 150)\n",
      "valshape (111, 1, 150)\n",
      "testshape (111, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE | Mean MASE |\n",
      "+==========+============+==============+===========+=============+===========+\n",
      "| Validate | 18.760     | 18.377       | 4.494     | 4.128       | 0.778     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "| Test     | 19.674     | 19.073       | 4.707     | 4.416       | 0.828     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "| dataset Name | Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+==============+=====+==========+============+==============+==================+\n",
      "| nn5          | 1   | 50       | 15         | Single       | 0                |\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "**************************************************\n",
      "dataset:  nn5\n",
      "len dataset 111\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (64935, 1, 150)\n",
      "valshape (111, 1, 150)\n",
      "testshape (111, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE | Mean MASE |\n",
      "+==========+============+==============+===========+=============+===========+\n",
      "| Validate | 18.479     | 17.738       | 4.433     | 4.154       | 0.769     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "| Test     | 19.528     | 18.781       | 4.672     | 4.389       | 0.826     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "| dataset Name | Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+==============+=====+==========+============+==============+==================+\n",
      "| nn5          | 2   | 50       | 5          | Single       | 0                |\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "len dataset 111\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (64935, 1, 150)\n",
      "valshape (111, 1, 150)\n",
      "testshape (111, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE | Mean MASE |\n",
      "+==========+============+==============+===========+=============+===========+\n",
      "| Validate | 18.540     | 17.752       | 4.438     | 4.148       | 0.768     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "| Test     | 19.449     | 18.662       | 4.681     | 4.411       | 0.820     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "| dataset Name | Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+==============+=====+==========+============+==============+==================+\n",
      "| nn5          | 2   | 50       | 10         | Single       | 0                |\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "len dataset 111\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (64935, 1, 150)\n",
      "valshape (111, 1, 150)\n",
      "testshape (111, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE | Mean MASE |\n",
      "+==========+============+==============+===========+=============+===========+\n",
      "| Validate | 18.657     | 18.129       | 4.453     | 4.296       | 0.774     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "| Test     | 19.424     | 18.696       | 4.662     | 4.415       | 0.821     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "| dataset Name | Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+==============+=====+==========+============+==============+==================+\n",
      "| nn5          | 2   | 50       | 15         | Single       | 0                |\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "**************************************************\n",
      "dataset:  nn5\n",
      "len dataset 111\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (64935, 1, 150)\n",
      "valshape (111, 1, 150)\n",
      "testshape (111, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE | Mean MASE |\n",
      "+==========+============+==============+===========+=============+===========+\n",
      "| Validate | 18.581     | 18.232       | 4.448     | 4.135       | 0.772     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "| Test     | 19.273     | 18.309       | 4.668     | 4.402       | 0.814     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "| dataset Name | Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+==============+=====+==========+============+==============+==================+\n",
      "| nn5          | 3   | 50       | 5          | Single       | 0                |\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "len dataset 111\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (64935, 1, 150)\n",
      "valshape (111, 1, 150)\n",
      "testshape (111, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE | Mean MASE |\n",
      "+==========+============+==============+===========+=============+===========+\n",
      "| Validate | 18.665     | 17.910       | 4.443     | 4.175       | 0.774     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "| Test     | 19.792     | 18.935       | 4.687     | 4.379       | 0.834     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "| dataset Name | Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+==============+=====+==========+============+==============+==================+\n",
      "| nn5          | 3   | 50       | 10         | Single       | 0                |\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "len dataset 111\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (64935, 1, 150)\n",
      "valshape (111, 1, 150)\n",
      "testshape (111, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE | Mean MASE |\n",
      "+==========+============+==============+===========+=============+===========+\n",
      "| Validate | 18.719     | 18.475       | 4.496     | 4.265       | 0.776     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "| Test     | 19.354     | 18.760       | 4.643     | 4.341       | 0.816     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "| dataset Name | Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+==============+=====+==========+============+==============+==================+\n",
      "| nn5          | 3   | 50       | 15         | Single       | 0                |\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "**************************************************\n",
      "dataset:  nn5\n",
      "len dataset 111\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (64935, 1, 150)\n",
      "valshape (111, 1, 150)\n",
      "testshape (111, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE | Mean MASE |\n",
      "+==========+============+==============+===========+=============+===========+\n",
      "| Validate | 18.588     | 18.097       | 4.399     | 4.171       | 0.770     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "| Test     | 20.375     | 19.680       | 4.807     | 4.450       | 0.861     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "| dataset Name | Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+==============+=====+==========+============+==============+==================+\n",
      "| nn5          | 4   | 50       | 5          | Single       | 0                |\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "len dataset 111\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (64935, 1, 150)\n",
      "valshape (111, 1, 150)\n",
      "testshape (111, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE | Mean MASE |\n",
      "+==========+============+==============+===========+=============+===========+\n",
      "| Validate | 18.756     | 17.889       | 4.492     | 4.232       | 0.778     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "| Test     | 19.405     | 18.404       | 4.651     | 4.413       | 0.819     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "| dataset Name | Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+==============+=====+==========+============+==============+==================+\n",
      "| nn5          | 4   | 50       | 10         | Single       | 0                |\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "len dataset 111\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (64935, 1, 150)\n",
      "valshape (111, 1, 150)\n",
      "testshape (111, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "shapes (111, 56) (111, 56) (111, 791)\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE | Mean MASE |\n",
      "+==========+============+==============+===========+=============+===========+\n",
      "| Validate | 19.031     | 18.409       | 4.549     | 4.260       | 0.792     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "| Test     | 20.469     | 19.750       | 4.822     | 4.543       | 0.864     |\n",
      "+----------+------------+--------------+-----------+-------------+-----------+\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n",
      "| dataset Name | Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+==============+=====+==========+============+==============+==================+\n",
      "| nn5          | 4   | 50       | 15         | Single       | 0                |\n",
      "+--------------+-----+----------+------------+--------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"*\" * 50)\n",
    "    nn5(run = i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1459aa67",
   "metadata": {
    "_cell_guid": "4e093c68-1acf-44d0-a5f0-e3af5005c90a",
    "_uuid": "78ef933e-56a4-491e-a421-ce540a85cecd",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020224,
     "end_time": "2024-04-02T19:57:03.075176",
     "exception": false,
     "start_time": "2024-04-02T19:57:03.054952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b062d9",
   "metadata": {
    "papermill": {
     "duration": 0.020337,
     "end_time": "2024-04-02T19:57:03.116332",
     "exception": false,
     "start_time": "2024-04-02T19:57:03.095995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3792584,
     "sourceId": 6564428,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3887158,
     "sourceId": 6751870,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3894022,
     "sourceId": 6766042,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3894518,
     "sourceId": 6767050,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3914808,
     "sourceId": 6803760,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3922817,
     "sourceId": 6820448,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3949919,
     "sourceId": 6873766,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4353902,
     "sourceId": 7479663,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4354096,
     "sourceId": 7479915,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4354204,
     "sourceId": 7480064,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3863893,
     "sourceId": 7579986,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4449796,
     "sourceId": 7636003,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4450251,
     "sourceId": 7636606,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3785054,
     "sourceId": 7651500,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4665964,
     "sourceId": 7937079,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 157337232,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 157337836,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 157344405,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 167040943,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 168541686,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 168542239,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 35404.997645,
   "end_time": "2024-04-02T19:57:06.690126",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-02T10:07:01.692481",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
